<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Mains</title>
    <link rel="stylesheet" href="reco_mains.css">
    <script type="module" src="reco_mains.js"></script>
</head>
<body>

<nav>
<ul>
    <li><a href="homepage.html" class="lien_nav">
    Home
    </a></li>
    <li><a href="Mon_stage.html" class="lien_nav">
    Stage
    </a></li>
    <li><a href="Mon_parcours.html" class="lien_nav">
    Parcours
    </a></li>
    <li class="dropdown">
    <a href="projets.html" class="lien_nav">Projets</a>
        <ul class="dropdown-content">
            <li><a class="burger_box" href="projets.html#main">Reconnaissance de main</a></li>
            <li><a class="burger_box" href="projets.html#renforcement">Article : machine learning</a></li>
            <li><a class="burger_box" href="projets.html#sandbagging">LLM sandbagging</a></li>
            <li><a class="burger_box" href="projets.html#minecraft">Musée virtuel sur Minecraft</a></li>
            <li><a class="burger_box" href="projets.html#scraping">Scraping Goodreads</a></li>

        </ul>
    </li>
</ul>
</nav>

    <div class="container">
        <h1>Classifications des mains</h1>
        <H3>Voici une illustration de ce que fait mon modèle de reconnaissance de mains. Plus bas, vous pourrez voir le devlog de ce projet.</h3>
        
        <div class="controls-panel">
            <div class="control-group">
                <div class="control-item">
                    <label for="classCount">Nombre de classes :</label>
                    <input type="number" id="classCount" min="2" max="10" value="4">
                    <span>Entre 2 et 10</span>
                </div>
                <div class="control-item">
                    <label for="imagesPerClass">Mains par classe :</label>
                    <input type="number" id="imagesPerClass" min="1" max="10" value="5">
                    <span>Entre 1 et 10</span>
                </div>
                <div class="control-item">
                    <label for="imageSize">Taille des images :</label>
                    <input type="number" id="imageSize" min="50" max="120" value="70">
                    <span>Entre 50 et 120px</span>
                </div>
            </div>
            
            <div id="action-buttons">
                <button id="loadBtn">Charger les Images</button>
                
                <button id="clusterBtn" disabled>Former les Clusters</button>
            </div>
        </div>

        <div id="image-container"></div>
        
        <div id="status">
            <span id="statusText">Prêt à charger les images</span>
        </div>
    
        <button id="randomizeBtn" disabled style="display: none;">Disposition Aléatoire</button>
    
        <h2>Dev log</h2>
        

        <h3>14/09/2025 - 28/09/2025 :</h3>

        <p>J’ai recherché un dataset de mains sur Kaggle et j’ai trouvé Hands and Palm Dataset. Il y en avait d’autres, mais celui-ci proposait aussi des images du dos de la main, ce que je trouvais plus intéressant pour mon projet. Il contient 11 076 images, avec environ 30 images par personne. Le principal problème de ce dataset est que les photos sont normalisées : toutes prises sous le même angle, avec la même lumière et un fond blanc uniforme. Je m'en contenterai pour l'instant.</p>

        <p>Je suis ensuite allé sur la documentation de TensorFlow et j’ai suivi les étapes pour construire un CNN de base. Je l’ai adapté afin d’utiliser mes images de mains à la place du dataset d’exemple. J’ai lancé l’entraînement sur CPU avec 216 images (8 classes), obtenant une accuracy de 68%. J’ai voulu utiliser mon GPU, mais j’ai rencontré des problèmes de compatibilité : j’ai perdu une après-midi à mettre à jour les drivers et à installer les bonnes versions des bibliothèques. Finalement, j’ai compris que ma carte graphique était probablement trop ancienne.</p>

        <p>Après en avoir discuté, on m’a conseillé d’utiliser PyTorch. J’ai donc réécrit mon code pour garder la même architecture CNN mais en PyTorch, et tout a fonctionné correctement. Cette fois, j’ai utilisé 520 images réparties sur 19 classes. J’ai conservé le même modèle, mais j’ai appliqué une validation croisée (5 folds), puisque ma nouvelle configuration me permettait d’aller plus vite. J’ai obtenu une accuracy moyenne de 82%, avec un maximum de 87%. J’étais assez surpris de ces bons résultats, sachant que l’architecture restait simple. J'ai pensé que cela venait du fait que les photos du dataset étaient très normalisées.</p>

        <p>J’ai cherché d’autres datasets mieux adaptés à ma tâche, mais je n’en ai pas trouvé. J’en ai vu un intéressant, mais il nécessite de contacter un laboratoire à Singapour, et je ne sais pas si c’est pertinent pour mon projet.  
        En attendant, trois amis ont accepté que je prenne en photo leurs mains, auxquelles j’ai ajouté les miennes : environ 20 photos par main, avec des variations de lumière, d’angle, de fond et de position. J’ai entraîné le modèle sur ces données et obtenu une accuracy moyenne de 82%, avec un meilleur modèle à 94%. Comme j’avais peu de données, j'ai suspecté un overfitting et j’ai donc constitué un petit jeu de test avec 18 photos que le modèle n’avait jamais vues. Résultat : 100% d’accuracy sur le test, ce qui m’a encore une fois surpris.</p>

        <p>Prochainement, je compte prendre en photo les mains d’autres personnes, mais aussi refaire des photos des mêmes personnes à différents moments. Cela permettra de voir si l’intervalle de temps joue un rôle (ongles coupés, vernis, bagues, petites blessures, etc.). Avec plus de données et davantage de classes, la tâche deviendra sûrement plus difficile.</p>

        <h3>11/10/2025 :</h3>

        <p>Je n’arrive pas à obtenir assez de données en prenant moi-même les photos. Je suis donc parti à la recherche d’un nouveau jeu de données. J’ai trouvé sur Kaggle un ensemble d’images de mains dans différentes positions, à différentes distances et sous des éclairages variés. Les photos ne sont donc pas normalisées, sauf pour le fond qui reste toujours le même.  
        Je me suis dit que, de toute façon, le fond n’était pas très important, puisqu’il existe des modèles ROI (Region Of Interest) qui consistent à recadrer l’image avant de la donner au CNN, afin de ne conserver que la partie intéressante. C’est ce que je compte faire plus tard. À ce moment-là, que le fond soit uni ou non, le CNN ne le verra pas. Il faudra simplement vérifier que le modèle parvient bien à recadrer les mains lorsque le fond n’est pas uniforme, afin de pouvoir conclure que ce projet est viable.</p>

        <h3>12/10/2025 :</h3>

        <p>J’ai effectué quelques modifications sur le dataset pour pouvoir exécuter mon code plus facilement. Tout a très bien fonctionné et j’ai pu entraîner un modèle sur 1 612 images de mains (41 classes). En plus, les courbes de loss et d’accuracy sont plutôt jolies.  
        On obtient 98,76 % d’accuracy sur le jeu de test (319 bonnes réponses sur 323).  
        Grâce à ce nouveau dataset, je sais maintenant qu’il ne s’agit pas d’overfitting.</p>

        <p>En revanche, je me suis rendu compte qu’il y avait des mains droites et gauches pour une même personne. Je pensais que le modèle se basait sur les lignes de la main, mais celles-ci diffèrent entre la main droite et la main gauche, ce qui m’interroge.</p>

        <p>Pour la prochaine fois, il faudrait que j’essaie de placer les mains droites et gauches dans des classes différentes. Il faudrait aussi que je construise un modèle ROI ou que j’en trouve un adapté sur Internet. Je pense que c’est possible dans les bibliothèques TensorFlow ou PyTorch.</p>

        <div class="image">
            <img src="image_courbes.png" alt="image_test" width="500">
        </div>

        <h3>19/10/2025 :</h3>

        <p>J’ai longuement cherché des ROI ou un dataset sur lequel je puisse entraîner un modèle. Sur GitHub, j’ai finalement trouvé <em>“A robust algorithm for extracting ROI from palm images taken by mobile phone”</em>. C’était exactement ce qu’il me fallait ! J’ai tenté d’utiliser ce dépôt et ai rencontré de nombreux bugs. Il y avait des problèmes de version, car l’algorithme utilisait d’anciennes bibliothèques (ce dépôt a sept ans, en même temps…), mais surtout, il contenait du code en C++ qui faisait crasher mon kernel.</p>

        <p>Comme j’apprends le C++, je me suis dit que si le problème venait de Python, il me suffisait d’utiliser le C++. Quelle erreur ! Les bugs se sont enchaînés et j’ai encore perdu plusieurs heures. J’ai donc décidé de créer mon propre système d’extraction de ROI plutôt que d’utiliser un code obsolète.</p>

        <p>J’ai utilisé la bibliothèque CV2, qui permet de manipuler et traiter des images. Au départ, j’ai essayé de différencier la main du fond par la couleur, mais les contours étaient souvent trop proches de la main, avec des doigts rognés. J’ai ensuite utilisé les contrastes : c’était bien mieux, mais le seuil de contraste variait trop d’une image à l’autre. Je me suis alors dit qu’il serait judicieux de fusionner les deux méthodes. Cela fonctionnait encore mieux, mais le problème de seuil persistait : les contrastes devenaient trop imprécis ou, au contraire, trop précis selon la luminosité.</p>

        <p>Je me suis donc arrêté là pour aujourd’hui, et vous pouvez voir le résultat plus bas.  
        Il faudrait maintenant que je trouve un moyen de faire une moyenne de la luminosité au centre de la photo (c’est-à-dire mesurer à quel point les pixels sont clairs) afin d’adapter chaque image et d’obtenir un seuil cohérent. Je sais que c’est ainsi que les appareils photo ajustent les ISO (sensibilité à la lumière), donc cela ne devrait pas être insurmontable.</p>

        <div class="image">
            <img src="1main.JPG" alt="image_test" width="200">
            <img src="2mains.png" alt="image_test" width="600">
        </div>
        <p>Exemple d'image où ça n'a pas fonctionné :</p>
        <div class="image">
            <img src="3mains.png" alt="image_test" width="200">
        </div>
    </div>
</body>
</html>